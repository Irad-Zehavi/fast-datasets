{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patches - enhancing fastai's data classes\n",
    "> Using monkey patching to add functionality to fastai's native classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Sequence, Union\n",
    "from functools import partial\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_datasets.mnist import TinyMNIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def sublist(self: TfmdLists, indices: Iterable[int]) -> TfmdLists:\n",
    "    \"\"\"a sublist that maintains laziness\"\"\"\n",
    "    sub = self.new_empty()\n",
    "    sub.items = [self.items[i] for i in indices]\n",
    "\n",
    "    all_indices = L(range_of(self))\n",
    "    def subsplit(s):\n",
    "        split_idxs = set(all_indices[s])\n",
    "        return [i for i, j in enumerate(indices) if j in split_idxs]\n",
    "    sub.splits = [subsplit(s) for s in self.splits]\n",
    "    \n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = TfmdLists(list('abcd'), 'f({})'.format, splits=[[0, 1], [2, 3]])\n",
    "sub_l = l.sublist([1, 3])\n",
    "\n",
    "test_eq(sub_l, L('f(b)', 'f(d)'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each split is also intersected with the requested indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(sub_l.train, L('f(b)'))\n",
    "test_eq(sub_l.valid, L('f(d)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def sub_dsets(self: Datasets, indices: Iterable[int]):\n",
    "    return Datasets(tls=[t.sublist(indices) for t in self.tls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Datasets(list('abcd'), ['f({})'.format, 'g({})'.format], splits=[[0, 1], [2, 3]])\n",
    "sub_ds = ds.sub_dsets([1, 3])\n",
    "\n",
    "test_eq(sub_ds, L(('f(b)', 'g(b)'), ('f(d)', 'g(d)')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each split is also intersected with the requested indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(sub_ds.train, L([('f(b)', 'g(b)')]))\n",
    "test_eq(sub_ds.valid, L([('f(d)', 'g(d)')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def random_sub_dsets(self: Datasets, size, with_replacement=False, less_ok=False) -> Datasets:\n",
    "    if size == 0:\n",
    "        return self.subset([])\n",
    "    if len(self) < size:\n",
    "        assert less_ok\n",
    "        size = len(self)\n",
    "    sampler = random.choices if with_replacement else random.sample\n",
    "    indices = sampler(range(len(self)),  k=size)\n",
    "    return self.sub_dsets(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(ds.random_sub_dsets(2)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fail(partial(ds.random_sub_dsets, size=6, less_ok=False))\n",
    "test_eq(len(ds.random_sub_dsets(6, less_ok=True)), len(ds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating `TfmdList`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def subset(self: TfmdLists, i):\n",
    "    s = self._new(self._get(self.splits[i]), split_idx=i)\n",
    "    s.splits = [slice(None), []]  # fastai bugfix\n",
    "    return s\n",
    "\n",
    "@patch\n",
    "def __eq__(self: Union[Pipeline, Transform], other: Union[Pipeline, Transform]):\n",
    "    \"\"\"Needed to find shared transforms between TfmdLists\"\"\"\n",
    "    return type(self) == type(other) and self.__dict__ == other.__dict__\n",
    "\n",
    "@patch\n",
    "def __add__(l1: TfmdLists, l2: TfmdLists):\n",
    "    assert l1.split_idx == l2.split_idx\n",
    "\n",
    "    tfms1, tfms2 = copy(list(l1.tfms)), copy(list(l2.tfms))\n",
    "    merged_tfms = []\n",
    "    while tfms1 and tfms2 and tfms1[-1] == tfms2[-1]:\n",
    "        merged_tfms.insert(0, tfms1.pop())\n",
    "        tfms2.pop()\n",
    "    tfms1, tfms2 = Pipeline(tfms1), Pipeline(tfms2)\n",
    "\n",
    "    return TfmdLists(\n",
    "        [[i, item] for i, l in enumerate([l1, l2]) for item in l.items],\n",
    "        tfms=[lambda o: [tfms1, tfms2][o[0]](o[1]), *merged_tfms],\n",
    "        splits=[L(range_of(l1))[s1] + [i+len(l1) for i in L(range_of(l2))[s2]]\n",
    "                for s1, s2 in zip_longest(l1.splits, l2.splits, fillvalue=[])],\n",
    "        do_setup=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = TfmdLists(list('abc'), 'f({})'.format, splits=[[0, 1], [2]])\n",
    "l2 = TfmdLists(list('bcd'), 'g({})'.format, splits=[[0], [1, 2]])\n",
    "\n",
    "test_eq(l1 + l2, L('f(a)', 'f(b)', 'f(c)', 'g(b)', 'g(c)', 'g(d)'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also concatenates each split separtely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq((l1+l2).train, l1.train + l2.train)\n",
    "test_eq((l1+l2).valid, l1.valid + l2.valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shares common transform postfix to allow `show`ing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = TinyMNIST()\n",
    "concat_l = mnist.tls[0]+mnist.tls[0]\n",
    "show_at(concat_l, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating `Datasets`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def __add__(self: Datasets, other: Datasets):\n",
    "    assert len(self.tls) == len(other.tls)\n",
    "    return Datasets(tls=[t1 + t2 for t1, t2 in zip(self.tls, other.tls)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = Datasets(list('abc'), ['f1({})'.format, 'f2({})'.format], splits=[[0, 1], [2]])\n",
    "ds2 = Datasets(list('bcd'), ['g1({})'.format, 'g2({})'.format], splits=[[0], [1, 2]])\n",
    "\n",
    "test_eq(ds1 + ds2, L(('f1(a)', 'f2(a)'), ('f1(b)', 'f2(b)'), ('f1(c)', 'f2(c)'),\n",
    "                     ('g1(b)', 'g2(b)'), ('g1(c)', 'g2(c)'), ('g1(d)', 'g2(d)')))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also concatenates each split separtely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq((ds1+ds2).train, ds1.train + ds2.train)\n",
    "test_eq((ds1+ds2).valid, ds1.valid + ds2.valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtracting Sub`Datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __sub__(self: Datasets, other: Datasets):\n",
    "    assert self.tfms == other.tfms\n",
    "    assert set(other.items).issubset(self.items)\n",
    "    return self.sub_dsets([i for i, o in enumerate(self.items) if o not in set(other.items)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ds-sub_ds, L(('f(a)', 'g(a)'), ('f(c)', 'g(c)')))\n",
    "test_eq((ds-sub_ds).train, L([('f(a)', 'g(a)')]))\n",
    "test_eq((ds-sub_ds).valid, L([('f(c)', 'g(c)')]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch(as_prop=True)\n",
    "def i2t(self: Datasets):\n",
    "    assert self.n_inp == len(self.tls) - 1\n",
    "    return self.tls[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Datasets(list('abcd'), ['f({})'.format, 'g({})'.format, 'h({})'.format])\n",
    "test_eq(ds.i2t, L('h(a)', 'h(b)', 'h(c)', 'h(d)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch(as_prop=True)\n",
    "def by_target(self: Datasets) -> Dict[int, Datasets]:\n",
    "    if not hasattr(self, '_by_target'):\n",
    "        targets = [self.vocab[t] for t in tqdm(self.i2t, desc='Class map: scanning targets')]\n",
    "        class_map = groupby(enumerate(targets), key=1, val=0)\n",
    "        self._by_target = {c: self.sub_dsets(indices) for c, indices in tqdm(class_map.items(), desc='Class map: partitioning')}\n",
    "    return self._by_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Datasets(range(10), [noop, [lambda o: ['Even', 'Odd'][o%2], Categorize()]])\n",
    "test_eq(ds.by_target.keys(), ['Even', 'Odd'])\n",
    "test_eq(ds.by_target['Even'], L((i, ds.vocab.o2i['Even']) for i in [0, 2, 4, 6, 8]))\n",
    "test_eq(ds.by_target['Odd'],  L((i, ds.vocab.o2i['Odd'])  for i in [1, 3, 5, 7, 9]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common default parameters for dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ListToTuple(Transform):\n",
    "    \"\"\"Transforms lists to tuples, useful for fixing a bug in pytorch (pin_memory turns inner tuples into lists)\"\"\"\n",
    "    def encodes(self, o:list):\n",
    "        return tuple(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "dl_defaults = {'pin_memory': default_device() != torch.device('cpu'), 'device': default_device(),\n",
    "               'after_item': [ToTensor], 'after_batch': [ListToTuple, IntToFloatTensor]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convenience methods for creating loaders with `dl_defaults`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _dl_args(kwargs):\n",
    "    args = deepcopy(dl_defaults)\n",
    "    for event in ['after_item', 'after_batch']:\n",
    "        if event in kwargs:\n",
    "            tfms = kwargs[event]\n",
    "            args[event] += tfms if isinstance(tfms, Sequence) else [tfms]\n",
    "    return args\n",
    "\n",
    "\n",
    "@patch\n",
    "def dls(self: Datasets, **kwargs) -> DataLoaders:\n",
    "    \"\"\"Calls `Datasets.dataloaders` with defaults from `dl_defaults`\"\"\"\n",
    "    return self.dataloaders(**_dl_args(kwargs))\n",
    "\n",
    "\n",
    "@patch\n",
    "def dl(self: Datasets, **kwargs) -> DataLoader:\n",
    "    \"\"\"Creates a `DataLoader` (ignoring splits) with defaults from `dl_defaults`\"\"\"\n",
    "    return self._dl_type(self, **_dl_args(kwargs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small enough datasets, we might want to load all of it to memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def load(self: Datasets, **kwargs):\n",
    "    return first(self.dl(bs=len(self), **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = TinyMNIST()\n",
    "x, y = mnist.random_sub_dsets(10).load()\n",
    "test_eq(x.shape, [10, 3, 28, 28])\n",
    "test_eq(y.shape, [10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch(as_prop=True)\n",
    "def subsets(self: Datasets) -> TfmdLists:\n",
    "    \"\"\"Lazy list of a `Datasets`'s subsets\"\"\"\n",
    "    return TfmdLists(range(self.n_subsets), self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Datasets(list('abcd'), ['f({})'.format, 'g({})'.format], splits=[[0, 2], [1, 3]])\n",
    "test_eq(ds.subsets, L(ds.train, ds.valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch()\n",
    "def __repr__(self: Datasets):\n",
    "    return '['+'\\n'.join(repr(s) for s in self.subsets)+']' if self.split_idx is None else coll_repr(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def resplit(self: Datasets,\n",
    "            splits: Union[Callable, List[List[int]]]  # a splitter function or a list of splits\n",
    "            ):\n",
    "    \"\"\"Sets the splits of a `Datasets`\"\"\"\n",
    "    if isinstance(splits, Callable):\n",
    "        splits = splits(self)\n",
    "    for t in self.tls:\n",
    "        t.splits = splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Datasets(list('abcd'), ['f({})'.format, 'g({})'.format], splits=[[0, 2], [1, 3]])\n",
    "ds.resplit(EndSplitter(.75))\n",
    "\n",
    "test_eq(ds.splits, [[0], [1, 2, 3]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_datasets.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show dataset structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample a random subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = mnist.random_sub_dsets(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the class distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = L((c, len(s)) for c, s in mnist.by_target.items()).sorted(0)\n",
    "plt.bar(class_counts.itemgot(0), class_counts.itemgot(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use only the even digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evens = mnist.by_target['0'] + mnist.by_target['2'] + mnist.by_target['4'] + mnist.by_target['6'] + mnist.by_target['8']\n",
    "evens.dls().show_batch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop specific classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_than_7 = mnist - mnist.by_target['9'] - mnist.by_target['8'] - mnist.by_target['7']\n",
    "less_than_7.dl().show_batch(max_n=25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the mean sample from a specific class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threes_sample = mnist.by_target['3'].random_sub_dsets(20)\n",
    "show_image(threes_sample.load()[0].mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
